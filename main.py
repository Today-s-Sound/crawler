import os
from typing import List, Dict, Optional
from urllib.parse import urlparse

from sites.dongguk_sw_board import DonggukSwBoardCrawler
from sites.kbuwel_notice import KbuwelNoticeCrawler
from sites.ablenews import AbleNewsCrawler
from sites.kead_notice import KeadNoticeCrawler
from sites.silwel_notice import SilwelNoticeCrawler
from services.subscription_client import fetch_subscriptions
from services.notification_client import create_alert, update_subscription_last_seen
from services.summarizer import summarize


def filter_new_posts(posts: List[Dict], last_seen_post_id: Optional[str]) -> List[Dict]:
    """
    posts: ìµœì‹ â†’ì˜¤ë˜ëœ ìˆœ
    last_seen_post_id: Noneì´ë©´ 'ìƒˆë¡œ ë³¸ ê²Œ ì—†ë‹¤'ê³  ê°€ì •í•˜ê³ , ì´ë²ˆì—ëŠ” ìƒˆ ì•Œë¦¼ ì•ˆ ë§Œë“¦.
    return: ì§€ë‚œë²ˆ ì´í›„ ìƒˆë¡œ ì˜¬ë¼ì˜¨ ê²Œì‹œë¬¼ë“¤ (ì˜¤ë˜ëœâ†’ìµœì‹  ìˆœ)
    
    ì•ˆì „ ì¥ì¹˜:
    - last_seen_post_idê°€ í˜„ì¬ í˜ì´ì§€ì— ì—†ìœ¼ë©´ ë‘ ê°€ì§€ ê²½ìš°:
      1) last_seen_post_idê°€ í˜„ì¬ ê²Œì‹œê¸€ë“¤ë³´ë‹¤ ìµœì‹  â†’ 0ê°œ ë°˜í™˜ (ê²Œì‹œê¸€ ì‚­ì œ/ê³µì§€ ì „í™˜)
      2) last_seen_post_idê°€ í˜„ì¬ ê²Œì‹œê¸€ë“¤ë³´ë‹¤ ì˜¤ë˜ë¨ â†’ ìµœì‹  3ê°œë§Œ ë°˜í™˜ (í˜ì´ì§€ ë„˜ì–´ê°)
    """
    if last_seen_post_id is None:
        return []

    if not posts:
        return []

    # ë””ë²„ê¹…: í˜„ì¬ í˜ì´ì§€ì˜ ëª¨ë“  ê²Œì‹œê¸€ ID ì¶œë ¥
    post_ids = [p["id"] for p in posts]
    print(f"[filter_new_posts] ğŸ” í˜„ì¬ í˜ì´ì§€ ê²Œì‹œê¸€ ID: {post_ids[:5]}{'...' if len(post_ids) > 5 else ''}")
    print(f"[filter_new_posts] ğŸ” ì°¾ê³  ìˆëŠ” last_seen_post_id: {last_seen_post_id}")

    new_posts = []
    found = False
    
    for post in posts:
        if post["id"] == last_seen_post_id:
            found = True
            print(f"[filter_new_posts] âœ… last_seen_post_idë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤!")
            break
        new_posts.append(post)
    
    # last_seen_post_idë¥¼ ì°¾ì§€ ëª»í•œ ê²½ìš°
    if not found:
        print(f"[filter_new_posts] âš ï¸ last_seen_post_id={last_seen_post_id}ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        
        # ID ë¹„êµë¥¼ í†µí•œ íŒë‹¨ (ìˆ«ì IDì¸ ê²½ìš°ì—ë§Œ)
        try:
            last_id_num = int(last_seen_post_id)
            latest_id_num = int(posts[0]["id"])
            oldest_id_num = int(posts[-1]["id"])
            
            # last_seen_idê°€ í˜„ì¬ í˜ì´ì§€ì˜ ìµœì‹  ê²Œì‹œê¸€ë³´ë‹¤ í¬ê±°ë‚˜ ê°™ìŒ
            # â†’ ê²Œì‹œê¸€ì´ ì‚­ì œë˜ì—ˆê±°ë‚˜ ê³µì§€ë¡œ ì „í™˜ë¨
            # â†’ ìƒˆ ê²Œì‹œê¸€ ì—†ìŒ!
            if last_id_num >= latest_id_num:
                print(f"[filter_new_posts] ğŸ” last_seen_id({last_id_num}) >= latest_id({latest_id_num})")
                print(f"[filter_new_posts] âœ… ê²Œì‹œê¸€ì´ ì‚­ì œë˜ì—ˆê±°ë‚˜ ê³µì§€ë¡œ ì „í™˜ë¨. ìƒˆ ê²Œì‹œê¸€ ì—†ìŒ!")
                return []
            
            # last_seen_idê°€ í˜„ì¬ í˜ì´ì§€ì˜ ê°€ì¥ ì˜¤ë˜ëœ ê²Œì‹œê¸€ë³´ë‹¤ ì‘ìŒ
            # â†’ ë‘ ë²ˆì§¸ í˜ì´ì§€ë¡œ ë„˜ì–´ê°
            # â†’ ì•ˆì „ ì¥ì¹˜: ìµœì‹  3ê°œë§Œ ë°˜í™˜
            elif last_id_num < oldest_id_num:
                print(f"[filter_new_posts] ğŸ” last_seen_id({last_id_num}) < oldest_id({oldest_id_num})")
                print(f"[filter_new_posts] âš ï¸ ê²Œì‹œê¸€ì´ ë§ì´ ì˜¬ë¼ì™€ í˜ì´ì§€ê°€ ë„˜ì–´ê°. ìµœì‹  3ê°œë§Œ ë°˜í™˜")
                new_posts = new_posts[:3]
            
        except (ValueError, TypeError):
            # IDê°€ ìˆ«ìê°€ ì•„ë‹Œ ê²½ìš° (URL ë“±)
            # ë³´ìˆ˜ì ìœ¼ë¡œ ìµœì‹  3ê°œë§Œ ë°˜í™˜
            print(f"[filter_new_posts] âš ï¸ IDê°€ ìˆ«ìê°€ ì•„ë‹˜. ë³´ìˆ˜ì ìœ¼ë¡œ ìµœì‹  3ê°œë§Œ ë°˜í™˜")
            if len(new_posts) > 3:
                new_posts = new_posts[:3]

    new_posts.reverse()
    return new_posts

def keyword_match(keyword: Optional[str], text: str) -> bool:
    """
    í‚¤ì›Œë“œê°€ ì—†ìœ¼ë©´ False ë°˜í™˜(=ë§¤ì¹­ ì—†ìŒ).
    í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ ë‹¨ìˆœ í¬í•¨ ì—¬ë¶€ë¡œ ë§¤ì¹­ íŒë‹¨.
    - ìš”ì•½ì€ í•­ìƒ ìˆ˜í–‰í•˜ê³ ,
    - "í‚¤ì›Œë“œê°€ ìˆê³  + ë§¤ì¹­ëœ ê²½ìš°"ì—ë§Œ ì•Œë¦¼ì„ ìƒì„±í•˜ê¸° ìœ„í•´ ì‚¬ìš©.
    """
    if not keyword:
        return False
    return keyword in text  # ê°„ë‹¨í•œ í¬í•¨ ì—¬ë¶€ (ë‚˜ì¤‘ì— ê°œì„  ê°€ëŠ¥)


def get_crawler_for_subscription(sub: Dict):
    """
    êµ¬ë…ì˜ site_url(ë˜ëŠ” site_type)ì„ ë³´ê³  ì–´ë–¤ í¬ë¡¤ëŸ¬ë¥¼ ì“¸ì§€ ê²°ì •.
    - sw.dongguk.edu        â†’ DonggukSwBoardCrawler
    - web.kbuwel.or.kr      â†’ KbuwelNoticeCrawler
    - www.ablenews.co.kr    â†’ AbleNewsCrawler
    - www.kead.or.kr       â†’ KeadNoticeCrawler
    - www.silwel.or.kr     â†’ SilwelNoticeCrawler
    """
    site_type = sub.get("site_type")
    if site_type == "DONGGUK_SW":
        return DonggukSwBoardCrawler()
    if site_type == "KBUWEL":
        return KbuwelNoticeCrawler()
    if site_type == "ABLE_NEWS":
        return AbleNewsCrawler()
    if site_type == "KEAD":
        return KeadNoticeCrawler()
    if site_type == "SILWEL":
        return SilwelNoticeCrawler()

    # site_type ì´ ì—†ìœ¼ë©´ URL ë„ë©”ì¸ìœ¼ë¡œ ì¶”ë¡ 
    url = sub.get("site_url", "")
    host = urlparse(url).netloc
    if "sw.dongguk.edu" in host:
        return DonggukSwBoardCrawler()
    if "web.kbuwel.or.kr" in host:
        return KbuwelNoticeCrawler()
    if "ablenews.co.kr" in host:
        return AbleNewsCrawler()
    if "kead.or.kr" in host:
        return KeadNoticeCrawler()
    if "silwel.or.kr" in host:
        return SilwelNoticeCrawler()

    # ê¸°ë³¸ê°’: ë™êµ­ëŒ€ í¬ë¡¤ëŸ¬
    return DonggukSwBoardCrawler()


# â€œêµ¬ë… í•˜ë‚˜ì— ëŒ€í•´ â€˜ì´ë²ˆ í„´ì— ìƒˆë¡œ ìƒê¸´ ì•Œë¦¼â€™ì„ DBì— ìŒ“ëŠ” ë‹¨ìœ„ ì‘ì—…â€
def process_subscription(
    sub: Dict,
    crawler,
    posts: List[Dict],
    content_cache: Dict[str, str],
    summary_cache: Dict[str, str],
):
    # ì´ë¯¸ site_url ë‹¨ìœ„ë¡œ í¬ë¡¤ë§ëœ posts/ crawler ë¥¼ ì¬ì‚¬ìš©
    print(f"[Sub {sub['id']}] site_url={sub['site_url']}")
    print(f"[Sub {sub['id']}] crawler={type(crawler).__name__}")

    if not posts:
        return

    last_seen_id = sub.get("last_seen_post_id")
    latest_id = posts[0]["id"]
    
    # ë””ë²„ê¹…: last_seen_id í™•ì¸
    print(f"[Sub {sub['id']}] ğŸ” last_seen_id={last_seen_id}, latest_id={latest_id}")

    # ì²« ì‹¤í–‰: ê°€ì¥ ìµœì‹  ê²Œì‹œê¸€ 1ê°œë¥¼ ë°”ë¡œ ìš”ì•½Â·ì•Œë¦¼ìœ¼ë¡œ ë³´ë‚´ê³ , ê·¸ ê²Œì‹œê¸€ì„ ê¸°ì¤€ì ìœ¼ë¡œ ì„¤ì •.
    if last_seen_id is None:
        latest_post = posts[0]
        print(f"[Sub {sub['id']}] ì²« ì‹¤í–‰ - ìµœì‹  ê²Œì‹œê¸€ 1ê°œë¥¼ ìš”ì•½ ë° ì•Œë¦¼ ìƒì„± (post_id={latest_id})")

        cache_key = latest_post.get("id") or latest_post["url"]
        if not cache_key:
            print(f"[Sub {sub['id']}] ìºì‹œ í‚¤ê°€ ì—†ì–´ ìŠ¤í‚µí•©ë‹ˆë‹¤")
            update_subscription_last_seen(sub["id"], latest_id)
            return
        if cache_key in content_cache:
            content_raw = content_cache[cache_key]
        else:
            content_raw = crawler.fetch_post_content(latest_post["url"])
            content_cache[cache_key] = content_raw

        # ë³¸ë¬¸ì´ ë¹„ì–´ìˆìœ¼ë©´ ìŠ¤í‚µ (í¬ë¡¤ëŸ¬ê°€ ë³¸ë¬¸ ì˜ì—­ì„ ì°¾ì§€ ëª»í•œ ê²½ìš°)
        if not content_raw.strip():
            print(f"[Sub {sub['id']}] ë³¸ë¬¸ì´ ë¹„ì–´ìˆì–´ ìŠ¤í‚µí•©ë‹ˆë‹¤: {latest_post['url']}")
            update_subscription_last_seen(sub["id"], latest_id)
            return

        # í‚¤ì›Œë“œ ë§¤ì¹­ ì—¬ë¶€ (ìˆìœ¼ë©´ í¬í•¨ ì—¬ë¶€, ì—†ìœ¼ë©´ False)
        matched = keyword_match(sub.get("keyword"),
                                latest_post["title"] + " " + content_raw)

        # ìƒˆ ê¸€ì´ë©´ ìš”ì•½ì€ í•­ìƒ ìˆ˜í–‰ (ë™ì¼ ê²Œì‹œê¸€ì— ëŒ€í•´ì„œëŠ” summary_cache ë¡œ ì¬ì‚¬ìš©)
        if cache_key in summary_cache:
            summary = summary_cache[cache_key]
            print(f"[Sub {sub['id']}] ìš”ì•½ ìºì‹œ íˆíŠ¸: {cache_key}")
        else:
            summary = summarize(content_raw)
            # ìš”ì•½ ì‹¤íŒ¨ í‘œì‹œê°€ ì—†ëŠ” ê²½ìš°ì—ë§Œ ìºì‹œ (ì‹¤íŒ¨ ì‹œ ë‹¤ìŒ subscriptionì—ì„œ ì¬ì‹œë„)
            if "[ìš”ì•½ ìƒì„± ì‹¤íŒ¨]" not in summary:
                summary_cache[cache_key] = summary
                print(f"[Sub {sub['id']}] ìš”ì•½ ìºì‹œ ì €ì¥: {cache_key}")
            else:
                print(f"[Sub {sub['id']}] ìš”ì•½ ì‹¤íŒ¨, ìºì‹œ ì•ˆí•¨: {cache_key}")

        # ì–´ë–¤ ê¸€ì´ ì–´ë–¤ ìš”ì•½ìœ¼ë¡œ DBì— ë“¤ì–´ê°€ëŠ”ì§€ ëˆˆìœ¼ë¡œ í™•ì¸í•  ìˆ˜ ìˆê²Œ ë¡œê·¸ ì¶œë ¥
        print(f"\n[Sub {sub['id']}] ìš”ì•½ ëŒ€ìƒ ê²Œì‹œê¸€: {latest_post['title']}")
        print(f"[Sub {sub['id']}] ìš”ì•½ ë³¸ë¬¸ (ì• 300ì): {summary[:300]}")

        # ì•Œë¦¼ ìƒì„± ìš”ì²­ ë°ì´í„° ìƒì„± (ë©”íƒ€ë°ì´í„°ë¥¼ ëª¨ë‘ í¬í•¨)
        alert_payload = {
            "user_id": sub["user_id"],
            "subscription_id": sub["id"],
            "site_alias": sub.get("site_alias"),
            "site_post_id": latest_post["id"],
            "title": latest_post["title"],
            "url": latest_post["url"],
            "published_at": latest_post.get("date"),
            "content_raw": content_raw,     # ì›ë¬¸ ì „ì²´ í…ìŠ¤íŠ¸
            "content_summary": summary,     # ìš”ì•½ í…ìŠ¤íŠ¸
            "keyword_matched": matched,
        }

        create_alert(alert_payload)
        update_subscription_last_seen(sub["id"], latest_id)
        return

    new_posts = filter_new_posts(posts, last_seen_id)

    if not new_posts: 
        print(f"[Sub {sub['id']}] ìƒˆ ê²Œì‹œë¬¼ ì—†ìŒ")
        return

    print(f"[Sub {sub['id']}] ìƒˆ ê²Œì‹œë¬¼ {len(new_posts)}ê°œ")
    # ë””ë²„ê¹…: ìƒˆ ê²Œì‹œë¬¼ ID ëª©ë¡ ì¶œë ¥
    new_post_ids = [p["id"] for p in new_posts]
    print(f"[Sub {sub['id']}] ğŸ” ìƒˆ ê²Œì‹œë¬¼ ID: {new_post_ids}")

    for post in new_posts:  # ìƒˆë¡œ ì˜¬ë¼ì˜¨ ê²Œì‹œë¬¼ë“¤(ì—¬ëŸ¬ ê°œì¼ ìˆ˜ë„ ìˆìŒ)ì„ í•˜ë‚˜ì”© ìˆœíšŒ.
        cache_key = post.get("id") or post["url"]
        if not cache_key:
            print(f"[Sub {sub['id']}] ìºì‹œ í‚¤ê°€ ì—†ì–´ ìŠ¤í‚µí•©ë‹ˆë‹¤: {post['url']}")
            continue
        if cache_key in content_cache:
            content_raw = content_cache[cache_key]
        else:
            content_raw = crawler.fetch_post_content(post["url"])
            content_cache[cache_key] = content_raw

        # ë³¸ë¬¸ì´ ë¹„ì–´ìˆìœ¼ë©´ ì´ ê²Œì‹œê¸€ì€ ìŠ¤í‚µ (í•˜ì§€ë§Œ last_seen_idëŠ” ì—…ë°ì´íŠ¸)
        if not content_raw.strip():
            print(f"[Sub {sub['id']}] ë³¸ë¬¸ì´ ë¹„ì–´ìˆì–´ ìŠ¤í‚µí•©ë‹ˆë‹¤: {post['url']}")
            continue

        # í‚¤ì›Œë“œ ë§¤ì¹­ ì—¬ë¶€ (ìˆìœ¼ë©´ í¬í•¨ ì—¬ë¶€, ì—†ìœ¼ë©´ False)
        matched = keyword_match(sub.get("keyword"), post["title"] + " " + content_raw)

        # ìƒˆ ê¸€ì´ë©´ ìš”ì•½ì€ í•­ìƒ ìˆ˜í–‰ (ë™ì¼ ê²Œì‹œê¸€ì— ëŒ€í•´ì„œëŠ” summary_cache ë¡œ ì¬ì‚¬ìš©)
        if cache_key in summary_cache:
            summary = summary_cache[cache_key]
            print(f"[Sub {sub['id']}] ìš”ì•½ ìºì‹œ íˆíŠ¸: {cache_key}")
        else:
            summary = summarize(content_raw)
            # ìš”ì•½ ì‹¤íŒ¨ í‘œì‹œê°€ ì—†ëŠ” ê²½ìš°ì—ë§Œ ìºì‹œ (ì‹¤íŒ¨ ì‹œ ë‹¤ìŒ subscriptionì—ì„œ ì¬ì‹œë„)
            if "[ìš”ì•½ ìƒì„± ì‹¤íŒ¨]" not in summary:
                summary_cache[cache_key] = summary
                print(f"[Sub {sub['id']}] ìš”ì•½ ìºì‹œ ì €ì¥: {cache_key}")
            else:
                print(f"[Sub {sub['id']}] ìš”ì•½ ì‹¤íŒ¨, ìºì‹œ ì•ˆí•¨: {cache_key}")

        # ì–´ë–¤ ê¸€ì´ ì–´ë–¤ ìš”ì•½ìœ¼ë¡œ DBì— ë“¤ì–´ê°€ëŠ”ì§€ ëˆˆìœ¼ë¡œ í™•ì¸í•  ìˆ˜ ìˆê²Œ ë¡œê·¸ ì¶œë ¥
        print(f"\n[Sub {sub['id']}] ìš”ì•½ ëŒ€ìƒ ê²Œì‹œê¸€: {post['title']}")
        print(f"[Sub {sub['id']}] ìš”ì•½ ë³¸ë¬¸ (ì• 300ì): {summary[:300]}")

        # ì•Œë¦¼ ìƒì„± ìš”ì²­ ë°ì´í„° ìƒì„± (ë©”íƒ€ë°ì´í„°ë¥¼ ëª¨ë‘ í¬í•¨)
        alert_payload = {
            "user_id": sub["user_id"],
            "subscription_id": sub["id"],
            "site_alias": sub.get("site_alias"),
            "site_post_id": post["id"],
            "title": post["title"],
            "url": post["url"],
            "published_at": post.get("date"),
            "content_raw": content_raw,     # ì›ë¬¸ ì „ì²´ í…ìŠ¤íŠ¸
            "content_summary": summary,     # ìš”ì•½ í…ìŠ¤íŠ¸
            "keyword_matched": matched,
        }

        # í‚¤ì›Œë“œ ìœ ë¬´/ë§¤ì¹­ê³¼ ìƒê´€ì—†ì´ í•­ìƒ ìš”ì•½ + ì•Œë¦¼ ìƒì„±
        # (keyword_matched í”Œë˜ê·¸ëŠ” ì„œë²„/í”„ë¡ íŠ¸ì—ì„œ í•„í„°ë§Â·ìš°ì„ ìˆœìœ„ìš©ìœ¼ë¡œ ì‚¬ìš© ê°€ëŠ¥)
        create_alert(alert_payload)

    # ë§ˆì§€ë§‰ìœ¼ë¡œ last_seen_post_id ê°±ì‹ 
    update_subscription_last_seen(sub["id"], latest_id)


def main():
    subs = fetch_subscriptions()
    print(f"ì´ êµ¬ë… ìˆ˜: {len(subs)}")

    # site_url ê¸°ì¤€ìœ¼ë¡œ êµ¬ë…ë“¤ì„ ê·¸ë£¹í™”í•´ì„œ
    # ê°™ì€ ì‚¬ì´íŠ¸ëŠ” ëª©ë¡ í¬ë¡¤ë§ì„ í•œ ë²ˆë§Œ ìˆ˜í–‰í•˜ê³  ê²°ê³¼ë¥¼ ê³µìœ í•œë‹¤.
    groups: Dict[str, List[Dict]] = {}
    for sub in subs:
        site_url = sub["site_url"]
        groups.setdefault(site_url, []).append(sub)

    for site_url, site_subs in groups.items():
        # ëŒ€í‘œ êµ¬ë… í•˜ë‚˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì–´ë–¤ í¬ë¡¤ëŸ¬ë¥¼ ì“¸ì§€ ê²°ì •
        rep_sub = site_subs[0]
        crawler = get_crawler_for_subscription(rep_sub)

        print(f"\n[Site] site_url={site_url}, crawler={type(crawler).__name__}, subs={len(site_subs)}")

        # í•´ë‹¹ ì‚¬ì´íŠ¸ì— ëŒ€í•œ ê²Œì‹œê¸€ ëª©ë¡ì€ í•œ ë²ˆë§Œ í¬ë¡¤ë§
        posts = crawler.fetch_post_list(site_url)
        if not posts:
            print(f"[Site] site_url={site_url} ì—ì„œ ê²Œì‹œê¸€ì´ ì—†ìŠµë‹ˆë‹¤.")
            continue

        # ìƒì„¸ ë³¸ë¬¸/ìš”ì•½ë„ ì—¬ëŸ¬ êµ¬ë…ì—ì„œ ê³µìœ í•  ìˆ˜ ìˆë„ë¡ ìºì‹œ
        content_cache: Dict[str, str] = {}
        summary_cache: Dict[str, str] = {}

        for sub in site_subs:
            try:
                process_subscription(sub, crawler, posts, content_cache, summary_cache)
            except Exception as e:
                sub_id = sub.get('id', 'unknown') if 'sub' in locals() else 'unknown'
                print(f"[Sub {sub_id}] ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")

if __name__ == "__main__":
    main()

